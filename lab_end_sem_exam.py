# -*- coding: utf-8 -*-
"""Lab-end-sem-exam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r7Cv4ig0m9jHcSBxbrM08jpY-GCTUsw5
"""



"""**NAME: UTKARSH MAJUMDAR
AP20110010205**


"""

from sklearn.datasets import load_iris
iris = load_iris()
print(iris.DESCR)

import pandas as pd
data = pd.DataFrame(iris.data)
data.head()

data.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
data.head()

target = pd.DataFrame(iris.target)
target = target.rename(columns = {0: 'target'})
target.head()

df = pd.concat([data, target], axis = 1)
df.head()

df.dtypes
df.isnull().sum()
df.describe()

import seaborn as sns
sns.heatmap(df.corr(), annot = True);

X = df.copy()
y = X.pop('target')

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1, stratify = y)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)
X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)

df.target.value_counts(normalize= True)
import numpy as np

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
model.score(X_test, y_test)

from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X_train, y_train, cv=10)
print(np.mean(scores))

df_coef = pd.DataFrame(model.coef_, columns=X_train.columns)
df_coef

predictions = model.predict(X_test)
compare_df = pd.DataFrame({'actual': y_test, 'predicted': predictions})
compare_df = compare_df.reset_index(drop = True)
compare_df

from sklearn.metrics import confusion_matrix
pd.DataFrame(confusion_matrix(y_test, predictions, labels=[2, 1, 0]),index=[2, 1, 0], columns=[2, 1, 0])

from sklearn.metrics import classification_report
print(classification_report(y_test, predictions))

probs = model.predict_proba(X_test)
Y_pp = pd.DataFrame(model.predict_proba(X_test), 
             columns=['class_0_pp', 'class_1_pp', 'class_2_pp'])
Y_pp.head()

#DECISION TREES
from sklearn import tree
classifier_2=tree.DecisionTreeClassifier()

dec_x=iris.data
dec_y=iris.target

from sklearn.model_selection import train_test_split
dec_x_train,dec_x_test,dec_y_train,dec_y_test=train_test_split(dec_x,dec_y,test_size=.5)

classifier_2.fit(dec_x_train,dec_y_train)

dec_predictions=classifier_2.predict(dec_x_test)

compare_df2 = pd.DataFrame({'actual': dec_y_test, 'predicted': dec_predictions})
compare_df2 = compare_df2.reset_index(drop = True)
compare_df2

from sklearn.metrics import accuracy_score
print(accuracy_score(dec_y_test,dec_predictions))

from sklearn.metrics import classification_report
print(classification_report(dec_y_test, dec_predictions))

from sklearn.metrics import confusion_matrix
pd.DataFrame(confusion_matrix(dec_y_test, dec_predictions, labels=[2, 1, 0]),index=[2, 1, 0], columns=[2, 1, 0])

dec_y_test.shape

dec_predictions.shape

dec_x_test.shape



"""LOGISTIC REGRESSION PERFORMS BETTER THAN DECISION TREES.

"""

