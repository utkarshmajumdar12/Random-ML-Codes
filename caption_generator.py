# -*- coding: utf-8 -*-
"""caption-generator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SchJG1YVa9gFbqePwp2fQsUIS10G1gHa
"""

!pip install openai

from transformers import AutoProcessor, BlipForConditionalGeneration, AutoTokenizer
import openai
from itertools import cycle
from tqdm import tqdm
from PIL import Image
import torch
import os

processor =AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
tokenizer =AutoTokenizer.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def prediction(img_list):
  max_length = 16
  num_beams = 4
  gen_kwargs = {"max_length": max_length, "num_beams": num_beams}

  img=[]

  for image in tqdm(img_list):
    i_image = Image.open(image)
    if i_image.mode!="RGB":
      i_image=i_image.convert(mode="RGB")
    img.append(i_image)

    pixel_values = processor(images=img, return_tensors="pt").pixel_values
    pixel_values = pixel_values.to(device)

    output=model.generate(pixel_values, **gen_kwargs)

    predict=tokenizer.batch_decode(output, skip_special_tokens=True)
    predict=[pred.strip() for pred in predict]

    return predict

images=[
    
    'img1.jpg'
]
res = prediction(images)

res

